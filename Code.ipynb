{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a0429a",
   "metadata": {},
   "source": [
    "# Code for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec509fa",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74346d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, scipy.signal, simpleaudio as sa, matplotlib.pyplot as plt\n",
    "import librosa, tensorflow as tf, torch, torchaudio\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Dropout,\n",
    "                                     Flatten, Dense, BatchNormalization)\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,\n",
    "                                        ReduceLROnPlateau)\n",
    "from sklearn.metrics import (confusion_matrix, classification_report,\n",
    "                             ConfusionMatrixDisplay)\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.model_selection import learning_curve as sk_learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb2c2c",
   "metadata": {},
   "source": [
    "### Data Augmentation \n",
    "We start by augmenting the data with the goal of building a robust yet accurate model.  \n",
    "In this function we create all the variant we want to get our input files through similar to a rotation tree. \n",
    " \n",
    "We use:  \n",
    "- Pitch shift  \n",
    "-  Background noise  \n",
    "- Echo  \n",
    "- Low frequency rumble: change in tone + delay  \n",
    "\n",
    "All of this are things you may encounter in a real-life scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93278625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(y, sr):\n",
    "\n",
    "\n",
    "    aug = []\n",
    "    # Convert input to tensor for pitch shifting\n",
    "    y_tensor = torch.tensor(y).float()\n",
    "    if len(y_tensor.shape) == 1:\n",
    "        y_tensor = y_tensor.unsqueeze(0)\n",
    "    # 1-a Pitch ↑ 2 st\n",
    "    aug.append(torchaudio.functional.pitch_shift(\n",
    "        waveform=y_tensor,\n",
    "        sample_rate=sr,\n",
    "        n_steps=2\n",
    "    ).squeeze().numpy())\n",
    "    # 1-b Pitch ↓ 2 st\n",
    "    aug.append(torchaudio.functional.pitch_shift(\n",
    "        waveform=y_tensor,\n",
    "        sample_rate=sr,\n",
    "        n_steps=-2\n",
    "    ).squeeze().numpy())\n",
    "    # 2   Background-noise mix\n",
    "    if len(_noise_clips) > 0: \n",
    "        \n",
    "        noise_idx = np.random.randint(0, len(_noise_clips))\n",
    "        noise = _noise_clips[noise_idx]\n",
    "        if len(noise) < len(y):\n",
    "            noise = np.tile(noise, int(np.ceil(len(y)/len(noise))))[:len(y)]\n",
    "        else:\n",
    "            noise = noise[:len(y)]\n",
    "            \n",
    "        snr = 10 \n",
    "        rms_y = np.sqrt(np.mean(y**2))\n",
    "        rms_n = np.sqrt(np.mean(noise**2))\n",
    "        noise_scaled = noise * (rms_y / (10**(snr/20)) / (rms_n + 1e-6))\n",
    "        aug.append(np.clip(y + noise_scaled, -1.0, 1.0))\n",
    "\n",
    "    # 3   Far-field (-9 dB) + 40 ms echo\n",
    "    quiet = y * 0.35\n",
    "    echo = np.pad(quiet * 0.3, (int(0.04*sr), 0))[:len(y)]\n",
    "    aug.append(np.clip(quiet + echo, -1.0, 1.0))\n",
    "\n",
    "    #4 Low Rumble\n",
    "    rumble = np.random.randn(len(y)) * 0.004\n",
    "    b, a = scipy.signal.butter(4, 100/(sr/2), 'low')\n",
    "    rumble = scipy.signal.filtfilt(b, a, rumble)\n",
    "    aug.append(np.clip(y + rumble, -1.0, 1.0))\n",
    "\n",
    "    return aug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a102314",
   "metadata": {},
   "source": [
    "### Add White Noise\n",
    "- To add even more similarities to what one might expect in real life, we also decided to add white noise with the following finction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_noise_library(noise_dir):\n",
    "\n",
    "    noise_clips = []\n",
    "    if os.path.exists(noise_dir):\n",
    "        for fn in os.listdir(noise_dir):\n",
    "            if fn.lower().endswith(\".wav\"):\n",
    "                try:\n",
    "                    y, _ = librosa.load(os.path.join(noise_dir, fn), sr=22050)\n",
    "                    if len(y.shape) > 1:\n",
    "                        y = y.mean(axis=1)\n",
    "                    noise_clips.append(y)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading noise file {fn}: {e}\")\n",
    "    return noise_clips\n",
    "\n",
    "# load white noise\n",
    "NOISE_DIR = \"/Users/vijaysridhar/Documents/white noise\"\n",
    "_noise_clips = load_noise_library(NOISE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee1a1b",
   "metadata": {},
   "source": [
    "### Data Cleaning and standarization\n",
    "- Here we first grab our data and standarize it at the same frequency and duration (set later to 3s ) in the first function. \n",
    "\n",
    "- In the Second we use mel spectrogram which reads the input file like a heatmap showing how the energy in waveform chages over time and frequency.\n",
    "\n",
    "- We go for 128 mels for accuracy and final function here check all spectrograms are the same lenght."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400dce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_process_audio(file_path, sr=22050, duration=None):\n",
    "    y, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "    return y, sr\n",
    "\n",
    "def create_melspectrogram(y, sr, n_mels=128, n_fft=2048, hop_length=512):\n",
    "    mel_spect = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\n",
    "                                               n_fft=n_fft, hop_length=hop_length)\n",
    "    mel_spect_db = librosa.power_to_db(mel_spect, ref=np.max)\n",
    "    return mel_spect_db\n",
    "\n",
    "def pad_or_truncate(mel_spect, target_length):\n",
    "    if mel_spect.shape[0] > target_length:\n",
    "        return mel_spect[:target_length, :]\n",
    "    else:\n",
    "        pad_width = target_length - mel_spect.shape[0]\n",
    "        return np.pad(mel_spect, ((0, pad_width), (0, 0)), mode='constant')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ad447",
   "metadata": {},
   "source": [
    "### Data Preprocessing \n",
    "- We put to use the augmentation function we created before with 'augment=False'when we test data, or run the base model and turn it \"True\" when trianing our proper model.  \n",
    "\n",
    "- Other features here,the function essentially goes into the folder where we have our wav files, grabs them, augments them and createds 5 extra versions if feature is turn on,runs them through the mel spectrogram and save them as features.   \n",
    "  \n",
    "- The Second function is our standard train-test where we go for 60% train , 20% validation, 20% test. also for CNN, we need to change the shape of the split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b24775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_dataset(data_folder, classes, sr=22050, duration=3.0, n_mels=128, augment=False):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    target_length = int(duration * sr / 512) + 1\n",
    "\n",
    "    for class_index, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(data_folder, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            print(f\"Warning: Folder {class_path} not found.\")\n",
    "            continue\n",
    "        for file in os.listdir(class_path):\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(class_path, file)\n",
    "                y, sr_ret = load_and_process_audio(file_path, sr=sr, duration=duration)\n",
    "                if y is None:\n",
    "                    continue\n",
    "                # Original sample\n",
    "                mel_spec = create_melspectrogram(y, sr_ret, n_mels=n_mels)\n",
    "                mel_spec = mel_spec.T \n",
    "                mel_spec = pad_or_truncate(mel_spec, target_length)\n",
    "                features.append(mel_spec)\n",
    "                labels.append(class_index)\n",
    "                # augmented samples if enabled\n",
    "                if augment:\n",
    "                    for y_aug in augment_audio(y, sr_ret):\n",
    "                        mel_spec_aug = create_melspectrogram(y_aug, sr_ret, n_mels=n_mels)\n",
    "                        mel_spec_aug = mel_spec_aug.T\n",
    "                        mel_spec_aug = pad_or_truncate(mel_spec_aug, target_length)\n",
    "                        features.append(mel_spec_aug)\n",
    "                        labels.append(class_index)\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    return X, y\n",
    "# Train Test Split\n",
    "def prepare_data(X, y, test_split=0.2, val_split=0.2):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_split, \n",
    "                                                        random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_split, \n",
    "                                                      random_state=42, stratify=y_train)\n",
    "    num_classes = len(np.unique(y))\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_val   = to_categorical(y_val, num_classes)\n",
    "    y_temp  = to_categorical(y_temp, num_classes)\n",
    "    # channel dimension for CNN input\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_val   = X_val[..., np.newaxis]\n",
    "    X_temp  = X_temp[..., np.newaxis]\n",
    "    return (X_train, y_train), (X_val, y_val), (X_temp, y_temp) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de7bba0",
   "metadata": {},
   "source": [
    "### Building the model \n",
    "\n",
    "We build our CNN with tensor flow and keras we apply  \n",
    "- Conv2D: 16 filters, 3x3 to learn local patterns  \n",
    "-  Batchnorm: speed up training, stabilizes learning\n",
    "-  Maxpoolin: reduces size, make model look at the big picture\n",
    "-  Drouput: helps prevent overfitting  \n",
    "\n",
    "Then we apply flatten to convert 2d features into 1D vector. Followed by Dense, to build a fully connected layer  with 64 neurons. \n",
    "Another droput, and finally apply dense to get a probability for each class and 'softmax' for multiclass classification.  \n",
    "\n",
    "Afterwards, we use model.compile with adam optimizer so weight are adjusted automaticalle, categorical crossentropy becasue our labels are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2fc93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    reg = tf.keras.regularizers.l2(1e-4)\n",
    "    model = Sequential([\n",
    "        Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=reg,\n",
    "               input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=reg),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu', kernel_regularizer=reg),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.optimizer.learning_rate.numpy())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea7c8d",
   "metadata": {},
   "source": [
    "### Train the model \n",
    "- Here we construct the CNN as well as apply some neat feature such as eraly stopping to save time if model stops improving, ReduceLROnPlateau, this slows the learning rate if val loss plateaus, and lastly modelchekpoint to save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffa2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_folder, classes, model_path, sr=22050, duration=3.0,\n",
    "                n_mels=128, batch_size=32, epochs=30, augment=True):\n",
    "    print(\"Processing dataset...\")\n",
    "    X, y = process_audio_dataset(data_folder, classes, sr, duration, n_mels, augment=augment)\n",
    "    if len(X) == 0:\n",
    "        print(\"No audio files processed. Check dataset path and file formats.\")\n",
    "        return\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = prepare_data(X, y)\n",
    "    input_shape = X_train.shape[1:] \n",
    "    num_classes = y_train.shape[1]\n",
    "    \n",
    "    print(\"Building and training the model...\")\n",
    "    model = build_model(input_shape, num_classes)\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1),\n",
    "        ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "    \n",
    "    loss, acc = model.evaluate(X_test, y_test)\n",
    "    print(\"Test accuracy:\", acc)\n",
    "    model.save(model_path)\n",
    "    print(\"Model saved to\", model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03942f",
   "metadata": {},
   "source": [
    "### Play Chime sound as well as detecting trigger word\n",
    "- The key feature from this block are grabbing the test file and standarize it.  \n",
    "- Playing chime sound if our model predicts > 50% of trigger word in audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    " def infer_trigger_word(model, file_path, sr=22050, duration=3.0, n_mels=128):\n",
    "    y, sr_ret = load_and_process_audio(file_path, sr=sr, duration=duration)\n",
    "    if y is None:\n",
    "        print(\"Error: Could not load audio for inference.\")\n",
    "        return False\n",
    "    mel_spec = create_melspectrogram(y, sr_ret, n_mels=n_mels)\n",
    "    mel_spec = mel_spec.T\n",
    "    target_length = int(duration * sr / 512) + 1\n",
    "    mel_spec = pad_or_truncate(mel_spec, target_length)\n",
    "    X_in = np.expand_dims(mel_spec, axis=0)     \n",
    "    X_in = np.expand_dims(X_in, axis=-1)         \n",
    "    pred = model.predict(X_in)\n",
    "\n",
    "    trigger_prob = pred[0][1]\n",
    "    print(\"Trigger word probability:\", trigger_prob)\n",
    "    return trigger_prob >= 0.5  # threshold \n",
    "\n",
    "def play_chime(chime_file):\n",
    "    try:\n",
    "        wave_obj = sa.WaveObject.from_wave_file(chime_file)\n",
    "        play_obj = wave_obj.play()\n",
    "        play_obj.wait_done()\n",
    "    except Exception as e:\n",
    "        print(\"Error playing chime:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5bfc7e",
   "metadata": {},
   "source": [
    "### Testing on sentences \n",
    "- Here we apply a novel technique to test our model in longer audios but we break them down into 3 sec 'windows' and slide accros .5s as we check.\n",
    "- We also appply our standarization and the infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00966397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_infer(model, file_path, sr=22050,\n",
    "                         win_dur=3.0, hop_dur=0.5,\n",
    "                         n_mels=128, thresh=0.25):\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    win_len = int(win_dur * sr)\n",
    "    hop_len = int(hop_dur * sr)\n",
    "    target_len = int(win_dur * sr / 512) + 1\n",
    "\n",
    "    for start in range(0, len(y) - win_len + 1, hop_len):\n",
    "        chunk = y[start:start + win_len]\n",
    "        mel = create_melspectrogram(chunk, sr, n_mels).T\n",
    "        mel = pad_or_truncate(mel, target_len)\n",
    "        X = mel[np.newaxis, ..., np.newaxis]\n",
    "        prob = model.predict(X, verbose=0)[0][1]\n",
    "        print(f\"{start/sr:5.1f}s → prob={prob:.3f}\")  # Optional debug print\n",
    "        if prob >= thresh:\n",
    "            print(f\"Trigger word detected at ≈ {start/sr:.1f}s (prob={prob:.2f})\")\n",
    "            return True\n",
    "    print(\"Trigger word not detected.\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce96848e",
   "metadata": {},
   "source": [
    "### RUN the code\n",
    "- here depending on the mode 'train' or 'infer' it grabs the necessary files and do as we have explained beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a2d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set mode to either 'train' or 'infer'\n",
    "    mode = 'train'  \n",
    "\n",
    "   # Parameters\n",
    "    SAMPLE_RATE = 22050\n",
    "    DURATION = 3.0\n",
    "    N_MELS = 128\n",
    "    MODEL_PATH = \"/Users/vijaysridhar/Documents/trigger_model.keras\"\n",
    "    \n",
    "    if mode == 'train':\n",
    "        # Path to folder (with subfolders for each class)\n",
    "        DATA_FOLDER = \"/Users/vijaysridhar/Documents/WAV\"  \n",
    "        CLASSES = [\"negative\", \"activate\"]  \n",
    "        # Automatically set augmentation true when trianing, turn off for base model\n",
    "        train_model(DATA_FOLDER, CLASSES, MODEL_PATH, sr=SAMPLE_RATE, duration=DURATION, n_mels=N_MELS, augment=True)\n",
    "    elif mode == 'infer':\n",
    "        AUDIO_FILE = \"/Users/vijaysridhar/Documents/inference/inference_test.wav\"\n",
    "        CHIME_FILE = \"/Users/vijaysridhar/Documents/inference/chime.wav\"\n",
    "        model = load_model(MODEL_PATH)\n",
    "        detected = sliding_window_infer(\n",
    "            model,\n",
    "            AUDIO_FILE,\n",
    "            sr=SAMPLE_RATE,\n",
    "            win_dur=3.0,\n",
    "            hop_dur=0.5,\n",
    "            n_mels=N_MELS,\n",
    "            thresh=0.25)\n",
    "        if detected:\n",
    "            print(\"Trigger word detected! Playing chime.\")\n",
    "            play_chime(CHIME_FILE)\n",
    "        else:\n",
    "            print(\"Trigger word not detected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a771d",
   "metadata": {},
   "source": [
    "### Visuals\n",
    "Spectrogram, Classification report, ROC curve, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5115b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_diagram(model, out_path=\"/Users/vijaysridhar/Documents/model_graph.png\"):\n",
    "\n",
    "    try:\n",
    "        plot_model(model, to_file=out_path, show_shapes=True, dpi=120)\n",
    "        print(f\"[VIS] model diagram  →  {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"[VIS] could not save model diagram:\", e)\n",
    "\n",
    "def plot_training_history(history, out_path=\"/Users/vijaysridhar/Documents/train_curves.png\"):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    ax[0].plot(history.history[\"accuracy\"], label=\"train\")\n",
    "    ax[0].plot(history.history[\"val_accuracy\"], label=\"val\")\n",
    "    ax[0].set_title(\"Accuracy\"); ax[0].legend()\n",
    "    ax[1].plot(history.history[\"loss\"], label=\"train\")\n",
    "    ax[1].plot(history.history[\"val_loss\"], label=\"val\")\n",
    "    ax[1].set_title(\"Loss\");  ax[1].legend()\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close(fig)\n",
    "    print(f\"[VIS] learning curves →  {out_path}\")\n",
    "\n",
    "def plot_confusion_matrix(model, X, y, class_names, out_path=\"/Users/vijaysridhar/Documents/conf_mat.png\"):\n",
    "    y_pred = np.argmax(model.predict(X, verbose=0), axis=1)\n",
    "    cm = confusion_matrix(y, y_pred, normalize=\"true\")\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False, values_format=\".2f\")\n",
    "    ax.set_title(\"Normalized Confusion Matrix\")\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close(fig)\n",
    "    print(f\"[VIS] confusion matrix →  {out_path}\")\n",
    "\n",
    "def print_classif_report(model, X, y, class_names):\n",
    "    y_pred = np.argmax(model.predict(X, verbose=0), axis=1)\n",
    "    rep = classification_report(y, y_pred, target_names=class_names, digits=3)\n",
    "    print(\"\\n=== Classification report ===\\n\", rep)\n",
    "\n",
    "def show_random_spectrogram(mel_batch, out_path=\"/Users/vijaysridhar/Documents/rand_spec.png\"):\n",
    "    \n",
    "    idx = np.random.randint(len(mel_batch))\n",
    "    S = mel_batch[idx, :, :, 0].T  \n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(S, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.colorbar(); plt.title(\"Training log-Mel spectrogram\")\n",
    "    plt.xlabel(\"Time-frames\"); plt.ylabel(\"Mel bins\")\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()\n",
    "    print(f\"[VIS] spectrogram snapshot →  {out_path}\")\n",
    "\n",
    "def plot_roc_curve(model, X, y, out_path=\"/Users/vijaysridhar/Documents/roc_curve.png\"):\n",
    "    y_score = model.predict(X, verbose=0)[:, 1]\n",
    "    y_true = y\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve'); plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()\n",
    "    print(f\"[VIS] ROC curve →  {out_path}\")\n",
    "\n",
    "def plot_pr_curve(model, X, y, out_path=\"/Users/vijaysridhar/Documents/pr_curve.png\"):\n",
    "    y_score = model.predict(X, verbose=0)[:, 1]\n",
    "    y_true = y\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.plot(recall, precision, color='green', lw=2)\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()\n",
    "    print(f\"[VIS] PR curve →  {out_path}\")\n",
    "\n",
    "\n",
    "def plot_true_learning_curve(X, y, build_fn, out_path=\"/Users/vijaysridhar/Documents/learning_curve.png\"):\n",
    "    # \n",
    "    class KerasWrapper(BaseEstimator, ClassifierMixin):\n",
    "        def __init__(self):\n",
    "            self.model = None\n",
    "            self.n_classes = len(np.unique(y))  \n",
    "            \n",
    "        def fit(self, X, y):\n",
    "            \n",
    "            X_reshaped = X.reshape(-1, 130, 128, 1)\n",
    "       \n",
    "            y_one_hot = to_categorical(y, num_classes=self.n_classes)\n",
    "            self.model = build_fn()\n",
    "            self.model.fit(X_reshaped, y_one_hot, \n",
    "                          epochs=10, batch_size=32, verbose=0)\n",
    "            return self\n",
    "            \n",
    "        def predict(self, X):\n",
    "    \n",
    "            X_reshaped = X.reshape(-1, 130, 128, 1)\n",
    "            return np.argmax(self.model.predict(X_reshaped, verbose=0), axis=1)\n",
    "            \n",
    "        def predict_proba(self, X):\n",
    "\n",
    "            X_reshaped = X.reshape(-1, 130, 128, 1)\n",
    "            return self.model.predict(X_reshaped, verbose=0)\n",
    "\n",
    "\n",
    "    if len(X.shape) == 4:  \n",
    "        X_input = X\n",
    "    else:  \n",
    "        X_input = X.reshape(-1, 130, 128, 1)\n",
    "\n",
    "\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        KerasWrapper(), X_input, y,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=StratifiedKFold(n_splits=3),\n",
    "        scoring='accuracy',\n",
    "        n_jobs=1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', label=\"Train\")\n",
    "    plt.plot(train_sizes, val_mean, 'o-', label=\"Validation\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[VIS] learning curve →  {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc218d9",
   "metadata": {},
   "source": [
    "# Apendix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edd5f58",
   "metadata": {},
   "source": [
    "### Negative Generation\n",
    "- We adjusted the number of samples as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9def1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from TTS.api import TTS\n",
    "\n",
    "output_dir = \"/Users/vijaysridhar/Documents/negative\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "negative_words = [\n",
    "    \"hello\", \"music\", \"light\", \"fan\", \"coffee\", \"open\", \"stop\", \"go\", \"yes\", \"no\",\n",
    "    \"volume\", \"rain\", \"snow\", \"call\", \"message\", \"mail\", \"turn\", \"lock\", \"alarm\", \"mute\",\n",
    "    \"joke\", \"news\", \"play\", \"pause\", \"resume\", \"sleep\", \"date\", \"timer\", \"time\", \"cancel\",\n",
    "    \"skip\", \"repeat\", \"weather\", \"next\", \"previous\", \"map\", \"drive\", \"car\", \"email\", \"morning\",\n",
    "    \"evening\", \"night\", \"wake\", \"walk\", \"run\", \"drink\", \"schedule\", \"note\", \"list\", \"shopping\",\n",
    "    \"reminder\", \"stock\", \"currency\", \"balance\", \"heat\", \"cold\", \"mode\", \"zoom\", \"book\",\n",
    "    \"door\", \"window\", \"curtain\", \"bed\", \"couch\", \"tv\", \"show\", \"series\", \"camera\", \"photo\",\n",
    "    \"record\", \"video\", \"chat\", \"search\", \"browse\", \"web\", \"scroll\", \"battery\", \"charge\", \"plug\",\n",
    "    \"wifi\", \"bluetooth\", \"hotspot\", \"flash\", \"lamp\", \"air\", \"brush\", \"bottle\", \"phone\", \"charger\",\n",
    "    \"temperature\", \"unlock\", \"send\", \"receive\", \"folder\", \"file\", \"help\", \"info\"\n",
    "]\n",
    "\n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False)\n",
    "male_speakers = [s for s in tts.speakers if \"male\" in s.lower()] or tts.speakers\n",
    "\n",
    "for i in range(500):\n",
    "    word = random.choice(negative_words)\n",
    "    speaker = random.choice(male_speakers)\n",
    "    out_path = os.path.join(output_dir, f\"neg_male_{speaker}_{i}.wav\")\n",
    "    print(f\"[{i+1}/500] {word} ({speaker})\")\n",
    "    tts.tts_to_file(text=word, speaker=speaker, language=\"en\", file_path=out_path)\n",
    "\n",
    "print(\"\\n Done generating negative word samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6b9ff",
   "metadata": {},
   "source": [
    "### Activate data generation\n",
    "- This script automatically generates 'activate' in different languages we adjust the number of samples as needed.\n",
    "- some of these also we can change the speaker to collect more voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1095f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from TTS.api import TTS\n",
    "\n",
    "\n",
    "output_dir = \"/Users/vijaysridhar/Documents/activate_tts\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Translations of the word \"activate\"\n",
    "translations = {\n",
    "    \"en\": \"activate\",\n",
    "    #\"es\": \"activar\",\n",
    "    #\"hi\": \"सक्रिय करें\",\n",
    "    #\"zh\": \"激活\",\n",
    "    #\"ar\": \"تفعيل\",\n",
    "    \"fr-fr\": \"activer\",\n",
    "    #\"ru\": \"активировать\",\n",
    "    \"pt-br\": \"ativar\"\n",
    "    #\"de\": \"aktivieren\",\n",
    "    #\"it\": \"attivare\",\n",
    "    #\"nl\": \"activeren\"\n",
    "}\n",
    "\n",
    "# Load multilingual TTS model\n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False)\n",
    "\n",
    "all_speakers = tts.speakers\n",
    "male_speakers = [s for s in all_speakers if \"male\" in s.lower()] or all_speakers\n",
    "\n",
    "for i in range(100):\n",
    "    lang = random.choice(list(translations.keys()))\n",
    "    word = translations[lang]\n",
    "    text = f\"The command is {word}.\"\n",
    "\n",
    "    speaker = random.choice(male_speakers)\n",
    "    out_path = os.path.join(output_dir, f\"{lang}_{speaker}_{i}.wav\")\n",
    "\n",
    "    print(f\"[{i+1}/100] Generating: {text} ({lang}, speaker: {speaker})\")\n",
    "    tts.tts_to_file(text=text, speaker=speaker, language=lang, file_path=out_path)\n",
    "\n",
    "print(f\"\\nfiles saved to: {os.path.abspath(output_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eb6fbb",
   "metadata": {},
   "source": [
    "### Generate more activates through pyTTSx3\n",
    "- we use more TTs in order to create more different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16821442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyttsx3\n",
    "\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "driver = getattr(engine, '_driver', None)\n",
    "if driver is not None and not hasattr(driver, '_current_text'):\n",
    "    setattr(driver, '_current_text', \"\")\n",
    "\n",
    "OUT_DIR = \"/Users/vijaysridhar/Documents/activate_tts/pyttsx3_TTS\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "voices = engine.getProperty('voices')\n",
    "print(f\"Found {len(voices)} voices; writing one 'activate' per voice...\")\n",
    "\n",
    "for idx, voice in enumerate(voices, start=1):\n",
    "    engine.setProperty('voice', voice.id)\n",
    "    out_path = os.path.join(OUT_DIR, f\"activate_{idx:03d}.wav\")\n",
    "    print(f\"[{idx:03d}/{len(voices):03d}] → {out_path}\")\n",
    "    engine.save_to_file(\"activate\", out_path)\n",
    "\n",
    "\n",
    "engine.runAndWait()\n",
    "engine.stop()\n",
    "\n",
    "print(\"Done! Check the 'activate_outputs' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebe5197",
   "metadata": {},
   "source": [
    "### With the following coquiTTs \n",
    "- we wanted to generate sentences to test our model in more realife scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from TTS.api import TTS\n",
    "\n",
    "OUTPUT_DIR = \"/Users/vijaysridhar/Documents/activate_tts/coquiTTS\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 50 distinct sentences containing the word \"activate\"\n",
    "sentences = [\n",
    "    \"Please activate the alarm before you go to bed.\",\n",
    "    \"Can you activate the sprinkler system for the garden?\",\n",
    "    \"He pressed the button to activate the emergency shutdown.\",\n",
    "    \"She needed a password to activate her new software license.\",\n",
    "    \"The fire marshal will activate the sprinklers in case of smoke.\",\n",
    "    \"To begin the experiment, you must activate the centrifuge.\",\n",
    "    \"We’ll activate your account once we’ve verified your email.\",\n",
    "    \"Don’t forget to activate the router after you plug it in.\",\n",
    "    \"The scientist hoped to activate the enzyme with heat.\",\n",
    "    \"You can activate night mode in the app’s settings.\",\n",
    "    \"Once you activate the coupon code, you’ll see the discount.\",\n",
    "    \"The lifeguard will activate the rescue buoy if someone drowns.\",\n",
    "    \"I need to activate airplane mode before the flight takes off.\",\n",
    "    \"To proceed, you must activate two-factor authentication.\",\n",
    "    \"The remote control can activate the garage door opener.\",\n",
    "    \"He whispered the secret word to activate the hidden door.\",\n",
    "    \"After installation, you’ll need to activate the new feature pack.\",\n",
    "    \"She turned the key to activate the electric fence.\",\n",
    "    \"They plan to activate the old mining equipment this Saturday.\",\n",
    "    \"The magician said a spell to activate the floating orb.\",\n",
    "    \"Please activate the backup generator in case of a power outage.\",\n",
    "    \"When you activate the floodlights, they will stay on for five minutes.\",\n",
    "    \"The app prompts you to activate location services.\",\n",
    "    \"He had to activate his membership before booking the class.\",\n",
    "    \"The teacher will activate the quiz at exactly 3:00 PM.\",\n",
    "    \"I clicked OK to activate the new privacy settings.\",\n",
    "    \"To activate the rocket’s engines, enter the launch code.\",\n",
    "    \"She needed to activate the warranty within 30 days.\",\n",
    "    \"The security guard will activate the lockdown procedure if needed.\",\n",
    "    \"Please activate the Bluetooth pairing on your device.\",\n",
    "    \"They decided to activate the emergency beacon in the storm.\",\n",
    "    \"The keynote speaker will activate the presentation with a clicker.\",\n",
    "    \"To activate the light show, dial the control panel.\",\n",
    "    \"He set a timer to activate the coffee maker at 6 AM.\",\n",
    "    \"We must activate the vaccination registry by next week.\",\n",
    "    \"Once you activate your card, you can start using it immediately.\",\n",
    "    \"The chef hit a switch to activate the meat grinder.\",\n",
    "    \"Don’t activate the self-destruct sequence by accident!\",\n",
    "    \"I love how you can activate voice commands hands-free.\",\n",
    "    \"The coach will activate the substitution at halftime.\",\n",
    "    \"Please activate your camera so we can see you.\",\n",
    "    \"She plans to activate her online store this weekend.\",\n",
    "    \"The technician will activate the alarm panel remotely.\",\n",
    "    \"To save power, the screen will only activate when you touch it.\",\n",
    "    \"We need to activate the heating system before winter arrives.\",\n",
    "    \"The archaeologist hopes to activate the ancient mechanism.\",\n",
    "    \"You must activate the trial period within 14 days.\",\n",
    "    \"When you activate your profile, people can start following you.\",\n",
    "    \"The diver will activate his flippers before entering the water.\"\n",
    "]\n",
    "\n",
    "\n",
    "tts = TTS(model_name=\"tts_models/en/vctk/vits\")\n",
    "\n",
    "voices = tts.speakers[:100]\n",
    "print(f\"Generating {len(sentences) * len(voices)} samples across {len(sentences)} sentences and {len(voices)} voices.\")\n",
    "\n",
    "\n",
    "for sent in sentences:\n",
    "    \n",
    "    safe_sent = (\n",
    "        sent.lower()\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"’\", \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\"?\", \"\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\".\", \"\")\n",
    "    )\n",
    "    for spk in voices:\n",
    "        \n",
    "        safe_spk = spk.replace(\"/\", \"_\")\n",
    "        filename = f\"{safe_sent}__{safe_spk}.wav\"\n",
    "        out_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        tts.tts_to_file(text=sent, speaker=spk, file_path=out_path)\n",
    "\n",
    "print(\"Finished generating sentence samples at:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca4d99",
   "metadata": {},
   "source": [
    "### Amazon polly\n",
    "- we also wanted to experiment with this powerful TTs to create even more samples, this one can do evrything we did before all in one prety much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "\n",
    "\n",
    "AWS_REGION = \"us-east-1\"     \n",
    "\n",
    "OUTPUT_DIR = \"/Users/vijaysridhar/Documents/activate_tts/polly\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "polly = boto3.client(\n",
    "    \"polly\",\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Get all voices\n",
    "    response = polly.describe_voices()\n",
    "    all_voices = response.get(\"Voices\", [])\n",
    "    print(f\"Found {len(all_voices)} total voices in Polly\")\n",
    "    \n",
    "    # Filter for English voices\n",
    "    english_voices = [v for v in all_voices if v[\"LanguageCode\"].startswith(\"en\")]\n",
    "    print(f\"\\nFound {len(english_voices)} English voices:\")\n",
    "    for v in english_voices:\n",
    "        print(f\"  - {v['Name']} ({v['LanguageName']}, gender={v['Gender']}, engine={v.get('SupportedEngines', ['standard'])[0]})\")\n",
    "    \n",
    "    # Filter for French voices\n",
    "    french_voices = [v for v in all_voices if v[\"LanguageCode\"].startswith(\"fr\")]\n",
    "    print(f\"\\nFound {len(french_voices)} French voices:\")\n",
    "    for v in french_voices:\n",
    "        print(f\"  - {v['Name']} ({v['LanguageName']}, gender={v['Gender']}, engine={v.get('SupportedEngines', ['standard'])[0]})\")\n",
    "    \n",
    "    # Filter for Portuguese voices\n",
    "    portuguese_voices = [v for v in all_voices if v[\"LanguageCode\"].startswith(\"pt\")]\n",
    "    print(f\"\\nFound {len(portuguese_voices)} Portuguese voices:\")\n",
    "    for v in portuguese_voices:\n",
    "        print(f\"  - {v['Name']} ({v['LanguageName']}, gender={v['Gender']}, engine={v.get('SupportedEngines', ['standard'])[0]})\")\n",
    "    \n",
    "    # Filter for Spanish voices\n",
    "    spanish_voices = [v for v in all_voices if v[\"LanguageCode\"].startswith(\"es\")]\n",
    "    print(f\"\\nFound {len(spanish_voices)} Spanish voices:\")\n",
    "    for v in spanish_voices:\n",
    "        print(f\"  - {v['Name']} ({v['LanguageName']}, gender={v['Gender']}, engine={v.get('SupportedEngines', ['standard'])[0]})\")\n",
    "    \n",
    "except (BotoCoreError, ClientError) as e:\n",
    "    print(\"Error fetching voices:\", e)\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "language_texts = {\n",
    "    \"en\": \"activate\",\n",
    "    \"fr\": \"activer\",\n",
    "    \"pt\": \"ativar\",\n",
    "    \"es\": \"activar\"\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\n--- Generating English samples ---\")\n",
    "for v in english_voices:\n",
    "    voice_id = v[\"Id\"]\n",
    "    \n",
    "    \n",
    "    supported_engines = v.get(\"SupportedEngines\", [\"standard\"])\n",
    "    engine = supported_engines[0]  \n",
    "    \n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"activate_en_{voice_id}.mp3\")\n",
    "\n",
    "    try:\n",
    "        resp = polly.synthesize_speech(\n",
    "            Text=language_texts[\"en\"],\n",
    "            OutputFormat=\"mp3\",\n",
    "            VoiceId=voice_id,\n",
    "            Engine=engine\n",
    "        )\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(resp[\"AudioStream\"].read())\n",
    "        print(f\"Saved '{language_texts['en']}' as {out_path}\")\n",
    "    except (BotoCoreError, ClientError) as e:\n",
    "        print(f\"Error synthesizing with voice {voice_id}:\", e)\n",
    "\n",
    "# Process French voices\n",
    "print(\"\\n--- Generating French samples ---\")\n",
    "for v in french_voices:\n",
    "    voice_id = v[\"Id\"]\n",
    "    \n",
    "    \n",
    "    supported_engines = v.get(\"SupportedEngines\", [\"standard\"])\n",
    "    engine = supported_engines[0] \n",
    "    \n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"activate_fr_{voice_id}.mp3\")\n",
    "\n",
    "    try:\n",
    "        resp = polly.synthesize_speech(\n",
    "            Text=language_texts[\"fr\"],\n",
    "            OutputFormat=\"mp3\",\n",
    "            VoiceId=voice_id,\n",
    "            Engine=engine\n",
    "        )\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(resp[\"AudioStream\"].read())\n",
    "        print(f\"Saved '{language_texts['fr']}' as {out_path}\")\n",
    "    except (BotoCoreError, ClientError) as e:\n",
    "        print(f\"Error synthesizing with voice {voice_id}:\", e)\n",
    "\n",
    "# Process Portuguese voices\n",
    "print(\"\\n--- Generating Portuguese samples ---\")\n",
    "for v in portuguese_voices:\n",
    "    voice_id = v[\"Id\"]\n",
    "    \n",
    "  \n",
    "    supported_engines = v.get(\"SupportedEngines\", [\"standard\"])\n",
    "    engine = supported_engines[0]  \n",
    "    \n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"activate_pt_{voice_id}.mp3\")\n",
    "\n",
    "    try:\n",
    "        resp = polly.synthesize_speech(\n",
    "            Text=language_texts[\"pt\"],\n",
    "            OutputFormat=\"mp3\",\n",
    "            VoiceId=voice_id,\n",
    "            Engine=engine\n",
    "        )\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(resp[\"AudioStream\"].read())\n",
    "        print(f\"Saved '{language_texts['pt']}' as {out_path}\")\n",
    "    except (BotoCoreError, ClientError) as e:\n",
    "        print(f\"Error synthesizing with voice {voice_id}:\", e)\n",
    "\n",
    "# Process Spanish voices\n",
    "print(\"\\n--- Generating Spanish samples ---\")\n",
    "for v in spanish_voices:\n",
    "    voice_id = v[\"Id\"]\n",
    "    \n",
    "  \n",
    "    supported_engines = v.get(\"SupportedEngines\", [\"standard\"])\n",
    "    engine = supported_engines[0]  \n",
    "    \n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"activate_es_{voice_id}.mp3\")\n",
    "\n",
    "    try:\n",
    "        resp = polly.synthesize_speech(\n",
    "            Text=language_texts[\"es\"],\n",
    "            OutputFormat=\"mp3\",\n",
    "            VoiceId=voice_id,\n",
    "            Engine=engine\n",
    "        )\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(resp[\"AudioStream\"].read())\n",
    "        print(f\"Saved '{language_texts['es']}' as {out_path}\")\n",
    "    except (BotoCoreError, ClientError) as e:\n",
    "        print(f\"Error synthesizing with voice {voice_id}:\", e)\n",
    "\n",
    "\n",
    "# Convert Files\n",
    "try:\n",
    "    from pydub import AudioSegment\n",
    "    print(\"\\nConverting MP3 files to WAV format...\")\n",
    "    \n",
    "    for filename in os.listdir(OUTPUT_DIR):\n",
    "        if filename.endswith(\".mp3\"):\n",
    "            mp3_path = os.path.join(OUTPUT_DIR, filename)\n",
    "            wav_path = os.path.join(OUTPUT_DIR, filename.replace(\".mp3\", \".wav\"))\n",
    "            \n",
    "            \n",
    "            sound = AudioSegment.from_mp3(mp3_path)\n",
    "            sound.export(wav_path, format=\"wav\")\n",
    "            print(f\"Converted {mp3_path} to {wav_path}\")\n",
    "            \n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\ninstall pydub\")\n",
    "    print(\"install ffmpeg\")\n",
    "\n",
    "print(\"\\nAll audio samples generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd96e59f",
   "metadata": {},
   "source": [
    "### Libri speech\n",
    "- We get more data negative from this pre trained library. This is not generated data, we just take some negatives and add them to our folder. We tried to get activates, but there were none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd713f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Paths\n",
    "ROOT = \"/Users/vijaysridhar/Documents/data\"\n",
    "TMP = \"/Users/vijaysridhar/Downloads\"\n",
    "ACTIVATE_DIR = os.path.join(ROOT, \"activate\")\n",
    "NEGATIVE_DIR = os.path.join(ROOT, \"negative\")\n",
    "\n",
    "def safe_mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def download_file(url, dest):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(dest, 'wb') as f:\n",
    "        shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "def fetch_speech_commands():\n",
    "    url = \"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
    "    tar_path = os.path.join(TMP, \"speech_commands.tar.gz\")\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(\"⬇Downloading Speech Commands...\")\n",
    "        download_file(url, tar_path)\n",
    "        print(\"Downloaded Speech Commands.\")\n",
    "    return tar_path\n",
    "\n",
    "def fetch_common_voice():\n",
    "    url = \"https://voice.mozilla.org/en/datasets\"  # Requires manual login/download\n",
    "    print(\"Common Voice needs manual download. Skipping automatic download.\")\n",
    "    return None\n",
    "\n",
    "def fetch_librispeech():\n",
    "    url = \"http://www.openslr.org/resources/12/dev-clean.tar.gz\"\n",
    "    tar_path = os.path.join(TMP, \"librispeech_dev_clean.tar.gz\")\n",
    "    if not os.path.exists(tar_path):\n",
    "        print(\"Downloading LibriSpeech (dev-clean)...\")\n",
    "        download_file(url, tar_path)\n",
    "        print(\"Downloaded LibriSpeech.\")\n",
    "    return tar_path\n",
    "\n",
    "def fetch_chime5():\n",
    "    print(\"CHiME-5 requires a restricted license. Skipping download.\")\n",
    "    return None\n",
    "\n",
    "def extract_tar(tar_path, extract_to):\n",
    "    if tar_path:\n",
    "        with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=extract_to)\n",
    "\n",
    "def process_audio_files(input_dir, output_dir, label=\"negative\", duration=2.0, max_files=200):\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(input_dir):\n",
    "        for f in filenames:\n",
    "            if f.endswith('.wav'):\n",
    "                files.append(os.path.join(root, f))\n",
    "    random.shuffle(files)\n",
    "    count = 0\n",
    "    for file_path in files:\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=22050)\n",
    "            if len(y) < sr * duration:\n",
    "                continue\n",
    "            start_idx = random.randint(0, len(y) - int(sr * duration))\n",
    "            clip = y[start_idx:start_idx + int(sr * duration)]\n",
    "            save_path = os.path.join(output_dir, f\"{label}_{count}.wav\")\n",
    "            sf.write(save_path, clip, sr)\n",
    "            count += 1\n",
    "            if count >= max_files:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file {file_path}: {e}\")\n",
    "\n",
    "def collect_clips():\n",
    "    \n",
    "    safe_mkdir(ACTIVATE_DIR)\n",
    "    safe_mkdir(NEGATIVE_DIR)\n",
    "\n",
    "    \n",
    "    sc_path = fetch_speech_commands()\n",
    "    if sc_path:\n",
    "        extract_tar(sc_path, TMP)\n",
    "        activate_files = [\n",
    "             \"activate\"  \n",
    "        ]\n",
    "        negative_files = [\"no\", \"off\", \"stop\", \"down\", \"left\", \"up\", \"right\",\"yes\", \"on\", \"go\"]\n",
    "\n",
    "        for word in activate_files:\n",
    "            word_dir = os.path.join(TMP, word)\n",
    "            if os.path.exists(word_dir):\n",
    "                process_audio_files(word_dir, ACTIVATE_DIR, label=\"activate\", max_files=50)\n",
    "        for word in negative_files:\n",
    "            word_dir = os.path.join(TMP, word)\n",
    "            if os.path.exists(word_dir):\n",
    "                process_audio_files(word_dir, NEGATIVE_DIR, label=\"negative\", max_files=50)\n",
    "\n",
    "    # 2. LibriSpeech \n",
    "    ls_path = fetch_librispeech()\n",
    "    if ls_path:\n",
    "        extract_tar(ls_path, TMP)\n",
    "        libri_dir = os.path.join(TMP, \"LibriSpeech\", \"dev-clean\")\n",
    "        if os.path.exists(libri_dir):\n",
    "            process_audio_files(libri_dir, NEGATIVE_DIR, label=\"negative\", max_files=200)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    safe_mkdir(TMP)\n",
    "    safe_mkdir(ROOT)\n",
    "    collect_clips()\n",
    "    print(\"Dataset creation complete. Check the 'data' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43d7ed",
   "metadata": {},
   "source": [
    "### Generate noise \n",
    "- Similarly we generate noise with the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fed356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"/Users/vijaysridhar/Documents/white noise\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "NOISE_SOURCES = {\n",
    "    \"airplane_cabin.wav\": \"https://actions.google.com/sounds/v1/ambiences/airplane_cabin_background.ogg\",\n",
    "    \"cafe_ambience.wav\": \"https://actions.google.com/sounds/v1/ambiences/coffee_shop.ogg\",\n",
    "    \"white_noise.wav\": \"https://actions.google.com/sounds/v1/ambiences/room_tone.ogg\"\n",
    "}\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': '*/*',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive'\n",
    "}\n",
    "\n",
    "def download_and_convert(name, url, sr=22050):\n",
    "    try:\n",
    "        print(f\"⬇️  Downloading {name}...\")\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Failed to download {name}: HTTP {r.status_code}\")\n",
    "            return False\n",
    "            \n",
    "        temp_path = os.path.join(OUTPUT_DIR, \"temp.ogg\")\n",
    "        with open(temp_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "        print(f\"🎧 Converting {name} to {sr} Hz mono WAV...\")\n",
    "        final_path = os.path.join(OUTPUT_DIR, name)\n",
    "        \n",
    "       \n",
    "        cmd = [\n",
    "            'ffmpeg', '-y',\n",
    "            '-i', temp_path,\n",
    "            '-ar', str(sr),        \n",
    "            '-ac', '1',            \n",
    "            '-t', '10',            \n",
    "            '-acodec', 'pcm_s16le', \n",
    "            final_path\n",
    "        ]\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"FFmpeg error: {result.stderr}\")\n",
    "            return False\n",
    "\n",
    "        # Clean up temp file\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "            \n",
    "        print(f\"✅ Saved: {final_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {name}: {str(e)}\")\n",
    "        return False\n",
    "    finally:\n",
    "        # Ensure temp file is cleaned up even if there's an error\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "\n",
    "success_count = 0\n",
    "for name, url in NOISE_SOURCES.items():\n",
    "    if download_and_convert(name, url):\n",
    "        success_count += 1\n",
    "\n",
    "print(f\"\\n Successfully processed {success_count}/{len(NOISE_SOURCES)} noise files in: {OUTPUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
